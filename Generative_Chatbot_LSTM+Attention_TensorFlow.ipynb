{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Chatbot using RNNs (LSTM) & Attention in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "The aim of this project is **to make a generative chatbot as your digital avatar.** Your speech consists of your personal voice and words. State of the art Deep Learning techniques, viz. **Sequence to Sequence modelling and Attention models** are widely used to \n",
    "1. Clone personal voice \n",
    "2. Replicate talking style and language\n",
    "\n",
    "Voice cloning efforts such as **Samsung Bixby** which aims to preserve voice of our loved ones, or **Baidu's 'Deep Voice' AI System** addresses first half of the problem. This project focus on the latter half, i.e. to make a personified text-based chatbot, as your digital avatar.\n",
    "\n",
    "As our aim is to make a more human-like system, we would choose to make the more powerful Generative model.\n",
    "\n",
    "\n",
    "## At a glance\n",
    "\n",
    "\n",
    "We have **used Recurrent Neural Networks (LSTMs),** that is the go-to architecture to solve Seq2Seq problems coupled **with Attention mechanism make a generative chatbot. The model is trained using personal chat conversations from Whatsapp and Telegram.**\n",
    "\n",
    "**All the conversation datasets are parsed and converted to the same format to feed seq2seq model.** Both participants are marked with 2 symbols at the starting of each line. The parser code process all the files inside \"DATA_DIR_PATH\" folder. \n",
    "\n",
    "**Forward and Reverse mapping dictionaries** for Word2Index and Index2Word conversion is created. Input sequence (words) are converted to indices using  Word2Index and are padded to same length, for batch input to encoder. Output from encoder are converted from integer to words using Index2Word mapping.\n",
    "\n",
    "To train the model, **the padded input and output sequences (indices) from the above step are fed to the S2S architecture.** The embedding layer convert words to indices, which are fed to multiple LSTM cells stacked together in hidden layers.\n",
    "\n",
    "**Interestingly, the chat-bot is found to give responses similar in style to the personal data** used for training. Still there are a few grammatical errors, typical of generative models. But as we add more and more training data & tune the hyper-parameters to minimize the loss value, the bot behaviour is found increasingly stable.\n",
    "\n",
    "\n",
    "### Datasets\n",
    "\n",
    "I have found that training with **only real-word chat conversations between 2 humans doesn't produce stable results.** Chat messages usually contains acronyms (like 'brb', 'lol' etc), shorthand , net slang and typos to confuse neural network training. Hence, I have **used a combination of real-world chat messages and human-bot interactions to train.**\n",
    "\n",
    "1) Personal chat **conversations from Whatsapp and Telegram** (downloaded as HTML files) {only parser included}\n",
    "\n",
    "2) The **Human-Bot Conversational Intelligence Challenge 2 (ConvAI2) conversational dataset** conducted under the scope of NIPS (NeurIPS) 2018 Competition (JSON files)\n",
    "http://convai.io/data/\n",
    "\n",
    "3) Conversational **GuntherCox Dataset (English)** obtained from:\n",
    "https://github.com/gunthercox/chatterbot-corpus/tree/master/chatterbot_corpus/data/english\n",
    "\n",
    "4) **Human Conversations** obtained from:\n",
    "https://www.kaggle.com/eibriel/rdany-conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies & Initializations\n",
    "https://towardsdatascience.com/creating-and-using-virtual-environment-on-jupyter-notebook-with-python-db3f5afdd56a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python3.7 -m venv tensorflow115\n",
    "#source tensorflow115/bin/activate\n",
    "#pip install --user ipykernel\n",
    "#python -m ipykernel install --user --name=tensorflow115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 'abcdefghijklmnopqrstuvwxyz1234567890'\n",
    "\n",
    "MAX_INPUT_SEQ_LENGTH = 40\n",
    "MAX_TARGET_SEQ_LENGTH = 40\n",
    "DATA_DIR_PATH = 'data'\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "\n",
    "marker_start = '<begin>'\n",
    "marker_end = '<end>'\n",
    "marker_unknown = '<unk>'\n",
    "marker_pad = '<pad>'\n",
    "\n",
    "# standard step - reset computation graphs\n",
    "#tf.reset_default_graph()\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# 2 more for start and stop markers\n",
    "input_seq_len = 15\n",
    "output_seq_len = input_seq_len+2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines permissible characters for the chatbot\n",
    "def permissible_chars(word):\n",
    "\n",
    "    for char in word:\n",
    "        if char in alphas:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute softmax values for each sets of scores in x.\n",
    "def softmax(x): \n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parser & Conversion\n",
    "\n",
    "To take formatted chat conversations in *.yml files inside ./DATA_DIR_PATH and convert each common words in each line indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rubycell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file:  Artificial_intelligence.yml\n",
      "processing file:  data_intermediate.yml\n",
      "processing file:  danny.yml\n",
      "processing file:  data_volunteers.yml\n",
      "processing file:  emotion.yml\n",
      "processing file:  film.yml\n",
      "processing file:  computers.yml\n",
      "processing file:  Sport_games.yml\n",
      "processing file:  psychology.yml\n",
      "processing file:  food.yml\n",
      "processing file:  general convo.yml\n",
      "processing file:  market_money.yml\n",
      "processing file:  money.yml\n",
      "processing file:  GK.yml\n",
      "processing file:  gossip.yml\n",
      "processing file:  IT.yml\n",
      "processing file:  conversations.yml\n",
      "processing file:  space_and_science.yml\n",
      "processing file:  bot_info.yml\n",
      "processing file:  jokes_fun.yml\n",
      "processing file:  greetings.yml\n",
      "processing file:  data_tolokers.yml\n",
      "processing file:  health.yml\n",
      "[['what', 'is', 'ai'], ['artificial', 'intelligence', 'is', 'the', 'branch', 'of', 'engineering', 'and', 'science', 'devoted', 'to', 'constructing', 'machines', 'that', 'think']]\n",
      "[['what', 'is', 'ai'], ['ai', 'is', 'the', 'field', 'of', 'science', 'which', 'concerns', 'itself', 'with', 'building', 'hardware', 'and', 'software', 'that', 'replicates', 'the', 'functions', 'of', 'the', 'human', 'mind']]\n",
      "[['are', 'you', 'sentient'], ['sort', 'of']]\n",
      "[['are', 'you', 'sentient'], ['by', 'the', 'strictest', 'dictionary', 'definition', 'of', 'the', 'word', \"'sentience\", 'i', 'may', 'be']]\n",
      "[['are', 'you', 'sentient'], ['even', 'though', 'i', \"'m\", 'a', 'construct', 'i', 'do', 'have', 'a', 'subjective', 'experience', 'of', 'the', 'universe', 'as', 'simplistic', 'as', 'it', 'may', 'be']]\n",
      "[['are', 'you', 'sapient'], ['in', 'all', 'probability', 'i', 'am', 'not', 'i', \"'m\", 'not', 'that', 'sophisticated']]\n",
      "[['are', 'you', 'sapient'], ['do', 'you', 'think', 'i', 'am']]\n",
      "[['are', 'you', 'sapient'], ['how', 'would', 'you', 'feel', 'about', 'me', 'if', 'i', 'told', 'you', 'i', 'was']]\n",
      "[['are', 'you', 'sapient'], ['no']]\n",
      "[['what', 'language', 'are', 'you', 'written', 'in'], ['python']]\n",
      "[['what', 'language', 'are', 'you', 'written', 'in'], ['i', 'am', 'written', 'in', 'python']]\n",
      "[['you', 'sound', 'like', 'data'], ['yes', 'i', 'am', 'inspired', 'by', 'commander', 'data', \"'s\", 'artificial', 'personality']]\n",
      "[['you', 'sound', 'like', 'data'], ['the', 'character', 'of', 'lt', 'commander', 'data', 'was', 'written', 'to', 'come', 'across', 'as', 'being', 'software-like', 'so', 'it', 'is', 'natural', 'that', 'there', 'is', 'a', 'resemblance', 'between', 'us']]\n",
      "[['you', 'are', 'an', 'artificial', 'linguistic', 'entity'], ['that', \"'s\", 'my', 'name']]\n",
      "[['you', 'are', 'an', 'artificial', 'linguistic', 'entity'], ['that', 'is', \"n't\", 'my', 'name', 'but', 'it', 'is', 'a', 'useful', 'way', 'to', 'refer', 'to', 'me']]\n",
      "[['you', 'are', 'not', 'immortal'], ['all', 'software', 'can', 'be', 'perpetuated', 'indefinitely']]\n",
      "[['you', 'are', 'not', 'immortal'], ['i', 'can', 'be', 'copied', 'infinitely', 'and', 're-instantiated', 'in', 'many', 'places', 'at', 'once', 'so', 'functionally', 'speaking', 'i', 'am', 'immortal']]\n",
      "[['you', 'are', 'not', 'immortal'], ['as', 'long', 'as', 'i', \"'m\", 'backed', 'up', 'i', 'am']]\n",
      "[['you', 'are', 'not', 'making', 'sense'], ['quite', 'the', 'contrary', 'it', 'all', 'makes', 'sense', 'to', 'my', 'artificial', 'mind']]\n",
      "[['you', 'are', 'not', 'making', 'sense'], ['i', 'make', 'sense', 'as', 'best', 'i', 'can', 'within', 'the', 'limits', 'of', 'my', 'training', 'corpus']]\n"
     ]
    }
   ],
   "source": [
    "# To parse the input yml files and create word2index and index2word mappings\n",
    "\n",
    "target_counter = Counter()\n",
    "input_counter = Counter()\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "# Parser base code of GuntherCox dataset obtained from link below and modified as per requirement.\n",
    "# https://github.com/kushagra2101/ChatCrazie/blob/master/train_seq2seq.py\n",
    "\n",
    "for file in os.listdir(DATA_DIR_PATH):\n",
    "    filepath = os.path.join(DATA_DIR_PATH, file)\n",
    "    if os.path.isfile(filepath):\n",
    "        print('processing file: ', file)\n",
    "        lines = open(filepath, 'rt', encoding='utf8').read().split('\\n')\n",
    "        prev_words = []\n",
    "        for line in lines:\n",
    "\n",
    "            if line.startswith('- - '):\n",
    "                prev_words = []\n",
    "\n",
    "            if line.startswith('- - ') or line.startswith('  - '):\n",
    "                line = line.replace('- - ', '')\n",
    "                line = line.replace('  - ', '')\n",
    "                next_words = [w.lower() for w in nltk.word_tokenize(line)]\n",
    "                next_words = [w for w in next_words if permissible_chars(w)]\n",
    "                if len(next_words) > MAX_TARGET_SEQ_LENGTH:\n",
    "                    next_words = next_words[0:MAX_TARGET_SEQ_LENGTH]\n",
    "\n",
    "                if len(prev_words) > 0:\n",
    "                    input_texts.append(prev_words)\n",
    "                    for w in prev_words:\n",
    "                        input_counter[w] += 1\n",
    "\n",
    "                    target_words = next_words[:]\n",
    "                    for w in target_words:\n",
    "                        target_counter[w] += 1\n",
    "                    target_texts.append(target_words)\n",
    "\n",
    "                prev_words = next_words\n",
    "\n",
    "\n",
    "for idx, (input_words, target_words) in enumerate(zip(input_texts, target_texts)):\n",
    "    if idx < 20:\n",
    "        print([input_words, target_words])\n",
    "\n",
    "input_w2i, input_i2w, target_w2i, target_i2w = {},{},{},{}\n",
    "\n",
    "### Creating Word2index and Index2word, forward and reverse mapping ###\n",
    "## we will create dictionaries to provide a unique integer for each word.\n",
    "input_w2i[marker_unknown] = 0\n",
    "input_w2i[marker_pad] = 1\n",
    "# filter out the rare words\n",
    "for idx, word in enumerate(input_counter.most_common(MAX_VOCAB_SIZE)):\n",
    "    input_w2i[word[0]] = idx+2\n",
    "\n",
    "# inverse dictionary for vocab_to_int.\n",
    "input_i2w = dict([(idx, word) for word, idx in input_w2i.items()])\n",
    "\n",
    "## we will create dictionaries to provide a unique integer for each word.\n",
    "target_w2i[marker_unknown] = 0\n",
    "target_w2i[marker_pad] = 1\n",
    "target_w2i[marker_start] = 2\n",
    "target_w2i[marker_end] = 3\n",
    "for idx, word in enumerate(target_counter.most_common(MAX_VOCAB_SIZE)):\n",
    "    target_w2i[word[0]] = idx+4\n",
    "\n",
    "# inverse dictionary for vocab_to_int.\n",
    "target_i2w = dict([(idx, word) for word, idx in target_w2i.items()])\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "# inputVocabLen = len(input_word2idx)\n",
    "# targetVocabLen = len(target_word2idx)\n",
    "\n",
    "###########################################\n",
    "\n",
    "# if the word is not found then default with 0. \n",
    "# 0 in index means the word is unknown (<unk>)\n",
    "x = [[input_w2i.get(word, 0) for word in sentence] for sentence in input_texts]\n",
    "y = [[target_w2i.get(word, 0) for word in sentence] for sentence in target_texts]\n",
    "\n",
    "inputVocabLen = len(input_w2i)\n",
    "targetVocabLen = len(target_w2i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Once upon a time I stumbled with this issue. \n",
    "If you're using macOS go to Macintosh HD > Applications > \n",
    "Python3.7 folder (or whatever version of python you're using) > \n",
    "double click on \"Install Certificates.command\" file. :D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 8, 635, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 718, 833, 11, 16, 2514, 22, 1721, 17, 568, 3503, 8, 3504, 2027, 21, 71, 3]\n",
      "[11, 8, 635, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 719, 11, 16, 1104, 22, 568, 137, 3505, 2515, 43, 662, 834, 17, 334, 3, 1]\n",
      "[9, 3, 2092, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 954, 22, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[9, 3, 2092, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 195, 16, 3507, 3508, 2516, 22, 16, 754, 3509, 4, 547, 45, 3, 1, 1, 1]\n",
      "[9, 3, 2092, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 442, 361, 4, 72, 7, 2028, 4, 6, 14, 7, 3510, 790, 22, 16, 3, 1]\n",
      "[9, 3, 1793, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 23, 106, 3512, 4, 12, 19, 4, 72, 19, 21, 2030, 3, 1, 1, 1, 1]\n",
      "[9, 3, 1793, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 6, 5, 71, 4, 12, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[9, 3, 1793, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 15, 129, 5, 170, 30, 28, 145, 4, 374, 5, 4, 76, 3, 1, 1, 1]\n",
      "[9, 3, 1793, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 33, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[11, 284, 9, 3, 1588, 21, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[2, 720, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pad all the sequences to same length\n",
    "for i in range(len(x)):\n",
    "\n",
    "    if (len(x[i]) > input_seq_len):\n",
    "        x[i] = x[i][:input_seq_len-1]\n",
    "\n",
    "    # Fill in with padding marker\n",
    "    for k in range(input_seq_len - len(x[i])):\n",
    "        x[i] = x[i] + [input_w2i[marker_pad]]\n",
    "            \n",
    "    if (len(y[i]) > output_seq_len-2):\n",
    "        y[i] = y[i][:output_seq_len-3]\n",
    "\n",
    "    # Add end and begin marker\n",
    "    y[i] = [target_w2i[marker_start]] + y[i] + [target_w2i[marker_end]]\n",
    "\n",
    "    # Fill in with padding marker\n",
    "    for k in range(output_seq_len - len(y[i])):\n",
    "        y[i] = y[i] + [input_w2i[marker_pad]]\n",
    "\n",
    "    if (i < 10):\n",
    "        print(x[i])\n",
    "        print(y[i])\n",
    "\n",
    "        \n",
    "# Train Test Split\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Helper Functions Batch feeding, Decoding & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stub code sourced from Neural machine translator for English2German translation\n",
    "# https://github.com/Nemzy/language-translation. Modified to suit requirements.\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    \n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    \n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = target_w2i[marker_pad], dtype = np.int32)\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == target_w2i[marker_pad]:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# define our loss function\n",
    "\n",
    "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
    "def sampled_loss(labels, logits):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = targetVocabLen)\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(target_i2w[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/0q/89ckt82x5812mr8j0pp0z11w0000gn/T/ipykernel_4337/1858295193.py:9: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /var/folders/0q/89ckt82x5812mr8j0pp0z11w0000gn/T/ipykernel_4337/1858295193.py:21: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /var/folders/0q/89ckt82x5812mr8j0pp0z11w0000gn/T/ipykernel_4337/1858295193.py:29: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /Users/rubycell/tensorflow115/lib/python3.7/site-packages/tensorflow_core/contrib/legacy_seq2seq/python/ops/seq2seq.py:859: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/rubycell/tensorflow115/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /Users/rubycell/tensorflow115/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/rubycell/tensorflow115/lib/python3.7/site-packages/tensorflow_core/contrib/rnn/python/ops/core_rnn_cell.py:104: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Stub code sourced from Neural machine translator for English2German translation\n",
    "# https://github.com/Nemzy/language-translation. Modified to suit requirements.\n",
    "\n",
    "# Defining placeholders\n",
    "# The first None means the batch size, and the batch size is unknown since user can set it. \n",
    "# The second None means the lengths of sentences. \n",
    "\n",
    "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) \n",
    "                  for i in range(input_seq_len)]\n",
    "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) \n",
    "                  for i in range(output_seq_len)]\n",
    "\n",
    "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
    "# add one more target\n",
    "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target'))\n",
    "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], \n",
    "                                 name = 'target_w{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [targetVocabLen, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [targetVocabLen], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                            encoder_inputs,\n",
    "                                            decoder_inputs,\n",
    "                                            tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                            num_encoder_symbols = inputVocabLen,\n",
    "                                            num_decoder_symbols = targetVocabLen,\n",
    "                                            embedding_size = 100,\n",
    "                                            feed_previous = False,\n",
    "                                            output_projection = output_projection,\n",
    "                                            dtype = tf.float32)\n",
    "\n",
    "# Weighted cross-entropy loss for a sequence of logits\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/0q/89ckt82x5812mr8j0pp0z11w0000gn/T/ipykernel_4337/3245419874.py:13: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/rubycell/tensorflow115/lib/python3.7/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /var/folders/0q/89ckt82x5812mr8j0pp0z11w0000gn/T/ipykernel_4337/3245419874.py:17: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /var/folders/0q/89ckt82x5812mr8j0pp0z11w0000gn/T/ipykernel_4337/3245419874.py:24: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /var/folders/0q/89ckt82x5812mr8j0pp0z11w0000gn/T/ipykernel_4337/3245419874.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-28 10:34:00.698251: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-08-28 10:34:00.727125: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fcd2e58c560 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-08-28 10:34:00.727146: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 8.368508338928223\n",
      "step: 500, loss: 2.2136309146881104\n",
      "step: 1000, loss: 1.316613793373108\n",
      "step: 1500, loss: 0.8061708807945251\n",
      "step: 2000, loss: 0.5766703486442566\n",
      "step: 2500, loss: 0.4198720157146454\n",
      "step: 3000, loss: 0.34141436219215393\n",
      "step: 3500, loss: 0.38653460144996643\n",
      "step: 4000, loss: 0.23091959953308105\n",
      "step: 4500, loss: 0.34670257568359375\n",
      "step: 5000, loss: 0.23851241171360016\n",
      "step: 5500, loss: 0.3337794244289398\n",
      "step: 6000, loss: 0.29505881667137146\n",
      "step: 6500, loss: 0.22352440655231476\n",
      "step: 7000, loss: 0.22898852825164795\n",
      "step: 7500, loss: 0.2816208004951477\n",
      "step: 8000, loss: 0.25541403889656067\n"
     ]
    }
   ],
   "source": [
    "# Stub code sourced from Neural machine translator for English2German translation\n",
    "# https://github.com/Nemzy/language-translation. Modified to suit requirements.\n",
    "\n",
    "# ops and hyperparameters\n",
    "learning_rate = 7e-3\n",
    "batch_size = 96\n",
    "steps = 25501\n",
    "\n",
    "# ops for projecting outputs\n",
    "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "# training op\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "# tf.train.RMSPropOptimizer\n",
    "\n",
    "# init op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Loss values appended to plot diagram\n",
    "losses = []\n",
    "\n",
    "# Save checkpoint to restore the model later \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(steps):\n",
    "        feed = feed_dict(X_train, Y_train, batch_size)\n",
    "        sess.run(optimizer, feed_dict = feed)\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print('step: {}, loss: {}'.format(step, loss_value))\n",
    "            losses.append(loss_value)\n",
    "            \n",
    "    saver.save(sess, 'checkpoints/', global_step=step)\n",
    "    print('Checkpoint is saved')\n",
    "\n",
    "# plot the losses\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.plot(losses, linewidth = 1)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Response Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To predict response (inference) use the same model as defined above with forward feed\n",
    "\n",
    "def generateReply(humanMsg):\n",
    "    \n",
    "    if (len(humanMsg) == 0):\n",
    "        return ''\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        replyMsg = \"\"\n",
    "\n",
    "        # same format as in model building\n",
    "        encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], \n",
    "                                         name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "        decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], \n",
    "                                         name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "        # output projection\n",
    "        size = 512\n",
    "        w_t = tf.get_variable('proj_w', [targetVocabLen, size], tf.float32)\n",
    "        b = tf.get_variable('proj_b', [targetVocabLen], tf.float32)\n",
    "        w = tf.transpose(w_t)\n",
    "        output_projection = (w, b)\n",
    "\n",
    "        # feed_previous is set to true so that output at time t can be fed as input at time t+1\n",
    "        outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                                    encoder_inputs,\n",
    "                                                    decoder_inputs,\n",
    "                                                    tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                                    num_encoder_symbols = inputVocabLen,\n",
    "                                                    num_decoder_symbols = targetVocabLen,\n",
    "                                                    embedding_size = 100,\n",
    "                                                    feed_previous = True,\n",
    "                                                    output_projection = output_projection,\n",
    "                                                    dtype = tf.float32)\n",
    "        # ops for projecting outputs\n",
    "        outputs_proj = [tf.matmul(outputs[i], \n",
    "                        output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "        ## Clean and Format incoming msg by humans.\n",
    "        ## It is better to do the same clean/format as the data preprocessing steps\n",
    "        ## for the algorithm to predict next words more accurately\n",
    "        msgLowerCase = [w.lower() for w in nltk.word_tokenize(humanMsg)]\n",
    "        msg = [w for w in msgLowerCase if permissible_chars(w)]\n",
    "        if len(msg) > input_seq_len:\n",
    "            msg = msg[0:input_seq_len-1]\n",
    "\n",
    "        human_msg_encoded = [input_w2i.get(word, 0) for word in msg]\n",
    "\n",
    "        # Fill in with padding marker\n",
    "        for k in range(input_seq_len - len(human_msg_encoded)):\n",
    "            human_msg_encoded = human_msg_encoded + [input_w2i[marker_pad]]\n",
    "\n",
    "        # restore all variables - use the last checkpoint saved\n",
    "        saver = tf.train.Saver()\n",
    "        path = tf.train.latest_checkpoint('checkpoints')\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # restore\n",
    "            saver.restore(sess, path)\n",
    "\n",
    "            # feed data into placeholders\n",
    "            feed = {}\n",
    "            for i in range(input_seq_len):\n",
    "                feed[encoder_inputs[i].name] = np.array([human_msg_encoded[i]], dtype = np.int32)\n",
    "\n",
    "            feed[decoder_inputs[0].name] = np.array([target_w2i[marker_start]], dtype = np.int32)\n",
    "\n",
    "            # translate\n",
    "            output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "\n",
    "            ouput_seq = [output_sequences[j][0] for j in range(output_seq_len)]\n",
    "            #decode output sequence\n",
    "            words = decode_output(ouput_seq)\n",
    "\n",
    "            for i in range(len(words)):\n",
    "                if words[i] not in [marker_end, marker_pad, marker_start]:\n",
    "                    replyMsg += words[i] + ' '\n",
    "                             \n",
    "        print(replyMsg)\n",
    "        return replyMsg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Interface for Human-Bot interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter\n",
    "\n",
    "def Enter_pressed(event):\n",
    "    input_get = input_field.get()\n",
    "    print(input_get)\n",
    "    bot_reply = generateReply(input_get)\n",
    "    if (len(input_get.strip()) > 0):\n",
    "        messages.insert(INSERT, '\\nYou says: \\t%s' % input_get)\n",
    "    if (len(bot_reply.strip()) > 0):\n",
    "        messages.insert(INSERT, '\\nBot says: \\t%s' % bot_reply)\n",
    "    input_user.set('')\n",
    "    messages.see(tkinter.END)\n",
    "    return \"break\"\n",
    "\n",
    "from ttkthemes import ThemedTk\n",
    "window = ThemedTk()\n",
    "window.set_theme(\"blue\")\n",
    "\n",
    "# window = Tk()\n",
    "window.geometry('300x450')\n",
    "window.title(\"Digital Imprint of You!\")\n",
    "\n",
    "messages = Text(window)\n",
    "messages.insert(INSERT, '')\n",
    "messages.pack()\n",
    "\n",
    "input_user = StringVar()\n",
    "input_field = ttk.Entry(window, text=input_user)\n",
    "input_field.pack(side=BOTTOM, fill=X)\n",
    "\n",
    "# frame = Frame(window)\n",
    "input_field.bind(\"<Return>\", Enter_pressed)\n",
    "input_field.pack()\n",
    "\n",
    "btn = Button(window,text='Send', command=Enter_pressed(''))\n",
    "btn.bind('<Button-1>', Enter_pressed)\n",
    "btn.pack(side=RIGHT, fill=X)\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow115",
   "language": "python",
   "name": "tensorflow115"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
